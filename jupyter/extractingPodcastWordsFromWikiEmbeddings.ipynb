{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aedf6807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWiki embeddings is 2 Gb file, too big to find each word and index it. So i decided to create new file with embeddings only with \\nwordd, that appear in my ds, otherwise it will be a big pain in the ass for quering and loading during server run\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "'''\n",
    "Wiki embeddings is 2 Gb file, too big to find each word and index it. So i decided to create new file with embeddings only with \n",
    "wordd, that appear in my ds, otherwise it will be a big pain in the ass for quering and loading during server run\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc5fa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words extracted and saved to wiki_word_only.txt\n"
     ]
    }
   ],
   "source": [
    "#let gpt do it\n",
    "file_path = '../datasets/wiki-news-300d-1M.vec'\n",
    "\n",
    "# Open the file and process each line\n",
    "with open(file_path, 'r',encoding='utf-8') as file:\n",
    "    # Create a list to store the words\n",
    "    words = []\n",
    "\n",
    "    for line in file:\n",
    "        # Split the line by spaces and append the first element (the word) to the list\n",
    "        word = line.split()[0]\n",
    "        words.append(word)\n",
    "\n",
    "# Now, `words` contains all the words from the first column\n",
    "# You can either process them directly in Python, or write them to a new file\n",
    "\n",
    "# To write them to a new file:\n",
    "output_path = 'wiki_word_only.txt'\n",
    "with open(output_path, 'w',encoding='utf-8') as output_file:\n",
    "    for word in words:\n",
    "        output_file.write(word + '\\n')\n",
    "\n",
    "print(\"Words extracted and saved to\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d8ae271",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki_word_only.txt','r',encoding='utf-8') as f:\n",
    "    word_wiki = []\n",
    "    for line in f:\n",
    "        word_wiki.append(line.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ccbf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999994"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8be26b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " 'the',\n",
       " '.',\n",
       " 'and',\n",
       " 'of',\n",
       " 'to',\n",
       " 'in',\n",
       " 'a',\n",
       " '\"',\n",
       " ':',\n",
       " ')',\n",
       " 'that',\n",
       " '(',\n",
       " 'is',\n",
       " 'for',\n",
       " 'on',\n",
       " '*',\n",
       " 'with',\n",
       " 'as',\n",
       " 'it',\n",
       " 'the',\n",
       " 'or',\n",
       " 'was',\n",
       " \"'\",\n",
       " \"'s\",\n",
       " 'by',\n",
       " 'from',\n",
       " 'at',\n",
       " 'i',\n",
       " 'this',\n",
       " 'you',\n",
       " '/',\n",
       " 'are',\n",
       " '=',\n",
       " 'not',\n",
       " '-',\n",
       " 'have',\n",
       " '?',\n",
       " 'be',\n",
       " 'which',\n",
       " ';',\n",
       " 'all',\n",
       " 'his',\n",
       " 'has',\n",
       " 'one',\n",
       " 'their',\n",
       " 'about',\n",
       " 'but',\n",
       " 'an',\n",
       " '|',\n",
       " 'said',\n",
       " 'more',\n",
       " 'page',\n",
       " 'he',\n",
       " 'your',\n",
       " 'will',\n",
       " 'its',\n",
       " 'so',\n",
       " 'were',\n",
       " 'had',\n",
       " 'also',\n",
       " 'only',\n",
       " 'if',\n",
       " 'time',\n",
       " 'some',\n",
       " 'people',\n",
       " 'like',\n",
       " 'who',\n",
       " 'them',\n",
       " 'other',\n",
       " 'they',\n",
       " 'when',\n",
       " 'wikipedia',\n",
       " 'article',\n",
       " 'what',\n",
       " '#',\n",
       " 'just',\n",
       " '!',\n",
       " 'any',\n",
       " 'after',\n",
       " 'there',\n",
       " 'would',\n",
       " 'can',\n",
       " 'in',\n",
       " 'her',\n",
       " 'talk',\n",
       " 'use',\n",
       " 'then',\n",
       " 'into',\n",
       " 'up',\n",
       " '...',\n",
       " 'we',\n",
       " 'over',\n",
       " 'my',\n",
       " 'out',\n",
       " 'here',\n",
       " 'now',\n",
       " 'because',\n",
       " 'do',\n",
       " 'work']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_wiki[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6296dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/finalAndRatings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ded8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = df['description']\n",
    "descriptions = [description.lower() for description in descriptions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aada798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\iliamak\\AppData\\Local\\Temp\\ipykernel_35312\\1638633855.py:1: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  re.split('\\W+',descriptions[0]) #works\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['current',\n",
       " 'and',\n",
       " 'classic',\n",
       " 'episodes',\n",
       " 'featuring',\n",
       " 'compelling',\n",
       " 'true',\n",
       " 'crime',\n",
       " 'mysteries',\n",
       " 'powerful',\n",
       " 'documentaries',\n",
       " 'and',\n",
       " 'in',\n",
       " 'depth',\n",
       " 'investigations',\n",
       " '']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\W+',descriptions[0]) #works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3861a246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\iliamak\\AppData\\Local\\Temp\\ipykernel_35312\\136247615.py:4: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  words = re.split('\\W+',description)\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "total_amount_words = 0\n",
    "for description in descriptions:\n",
    "    words = re.split('\\W+',description)\n",
    "    total_amount_words+= len(words)\n",
    "    for word in words:\n",
    "        unique_words.add(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "167286b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38613"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93a07e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 38613\n",
      "Total amount of words in all descriptions: 750425\n",
      "5.1454842256054905 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique words: {len(unique_words)}\")\n",
    "print(f\"Total amount of words in all descriptions: {total_amount_words}\")\n",
    "print(f\"{len(unique_words)*100/total_amount_words} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99df9215",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_indices = []\n",
    "for word in unique_words:\n",
    "    try:\n",
    "        unique_words_indices.append(word_wiki.index(word))\n",
    "    except ValueError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6f54010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words from all descriptions:  38613\n",
      "Total found words in wikipedia sheet:  31291\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique words from all descriptions: \",len(unique_words))\n",
    "print(\"Total found words in wikipedia sheet: \",len(unique_words_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c065891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict 'word':ind\n",
    "dict_word_ind = dict()\n",
    "for ind in unique_words_indices:\n",
    "    dict_word_ind[word_wiki[ind]] = ind\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1534e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_indices.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04dfdfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_unique_words_indices = unique_words_indices.copy() # was not sure if i will not delete sth, made a copy, not impornaant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5378de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki-news-300d-1M.vec', 'r',encoding='utf-8') as file:\n",
    "    word_embeddings_podcast_words = \"\"\n",
    "    #using pointer to make it faster\n",
    "    i = 0 \n",
    "\n",
    "    for index, line in enumerate(file):\n",
    "        if i < len(unique_words_indices) and index == unique_words_indices[i]:\n",
    "            word_embeddings_podcast_words += line\n",
    "            i += 1\n",
    "        if i >= len(unique_words_indices):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80c4381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/word_embedding_podcast_words.csv','w') as fajlik:\n",
    "    fajlik.write(word_embeddings_podcast_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a7e527c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31291"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_words_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee714f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
